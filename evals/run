#!/usr/bin/env ruby
# frozen_string_literal: true

# got to ensure evals are here
# rubocop:disable Discourse/Plugins/NamespaceConstants
EVAL_PATH = File.join(__dir__, "cases")
# rubocop:enable Discourse/Plugins/NamespaceConstants
#
if !Dir.exist?(EVAL_PATH)
  puts "Evals are missing, cloning from discourse/discourse-ai-evals"

  success =
    system("git clone git@github.com:discourse/discourse-ai-evals.git '#{EVAL_PATH}' 2>/dev/null")

  # Fall back to HTTPS if SSH fails
  if !success
    puts "SSH clone failed, falling back to HTTPS..."
    success = system("git clone https://github.com/discourse/discourse-ai-evals.git '#{EVAL_PATH}'")
  end

  if success
    puts "Successfully cloned evals repository"
  else
    abort "Failed to clone evals repository"
  end
end

discourse_path = File.expand_path(File.join(File.dirname(__FILE__), "../../.."))
# rubocop:disable Discourse/NoChdir
Dir.chdir(discourse_path)
# rubocop:enable Discourse/NoChdir

require "/home/sam/Source/discourse/config/environment"
require_relative "lib/llm"

# Set up command line argument parsing
require "optparse"
ENV["DISCOURSE_AI_NO_DEBUG"] = "1"

options = { eval_name: nil, model: nil, output_dir: File.join(discourse_path, "tmp", "evals") }

OptionParser
  .new do |opts|
    opts.banner = "Usage: evals/run [options]"

    opts.on("-e", "--eval NAME", "Name of the evaluation to run") do |eval_name|
      options[:eval_name] = eval_name
    end

    opts.on("-m", "--model NAME", "Model to evaluate") { |model| options[:model] = model }
  end
  .parse!

FileUtils.mkdir_p(options[:output_dir])

if options[:eval_name].nil?
  puts "Error: Must specify an evaluation name with -e or --eval"
  exit 1
end

cases_path = File.join(__dir__, "cases")

cases = Dir.glob(File.join(cases_path, "*/*.yml")).map { |f| [File.basename(f, ".yml"), f] }.to_h

if !cases.keys.include?(options[:eval_name])
  puts "Error: Unknown evaluation '#{options[:eval_name]}'"
  exit 1
end

llms = DiscourseAi::Evals::Llm.choose(options[:model])

if llms.empty?
  puts "Error: Unknown model '#{options[:model]}'"
  exit 1
end

eval_info = YAML.load_file(cases[options[:eval_name]]).symbolize_keys

# correct relative paths in args
begin
  eval_info[:args]&.each do |k, v|
    if k.to_sym == :path
      root = File.dirname(cases[options[:eval_name]])
      eval_info[:args][k] = File.join(root, v)
    end
  end
end

puts "Running evaluation '#{options[:eval_name]}'"

log_filename = "#{options[:eval_name]}-#{Time.now.strftime("%Y%m%d-%H%M%S")}.log"
logs_dir = File.join(__dir__, "log")
FileUtils.mkdir_p(logs_dir)
log_file = File.join(logs_dir, log_filename)

logger = Logger.new(File.open(log_file, "a"))

logger.info("Starting evaluation '#{options[:eval_name]}'")

Thread.current[:llm_audit_log] = logger

llms.each do |llm|
  if eval_info[:vision] && !llm.vision?
    logger.info("Skipping LLM: #{llm.name} as it does not support vision")
    next
  end

  logger.info("Evaluating with LLM: #{llm.name}")
  eval =
    llm.eval(
      type: eval_info[:type],
      args: eval_info[:args].symbolize_keys,
      expected_output: eval_info[:expected_output],
      expected_output_regex: eval_info[:expected_output_regex],
    )

  print "#{llm.name}: "
  if eval[:result] == :fail
    puts "Failed ðŸ”´"
    puts "---- Expected ----\n#{eval[:expected_output]}"
    puts "---- Actual ----\n#{eval[:actual_output]}"
    logger.error("Evaluation failed with LLM: #{llm.name}")
  elsif eval[:result] == :pass
    puts "Passed ðŸŸ¢"
    logger.info("Evaluation passed with LLM: #{llm.name}")
  else
    STDERR.puts "Error: Unknown result #{eval.inspect}"
    logger.error("Unknown result: #{eval.inspect}")
  end
end

puts
puts "Log file: #{log_file}"
